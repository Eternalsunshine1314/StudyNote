{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 数据下载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('MNIST_data/'):\n",
    "    os.mkdir('MNIST_data/')\n",
    "mnist=read_data_sets('MNIST_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用简单的网络来实现手写体的识别\n",
    "为了用python实现高效的数值计算，我们通常会使用函数库，比如NumPy，会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果你用GPU来进行外部计算，这样的开销会更大。用分布式的计算方式，也会花费更多的资源用来传输数据。\n",
    "\n",
    "TensorFlow也把复杂的计算放在python之外完成，但是为了避免前面说的那些开销，它做了进一步完善。Tensorflow不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 train loss:230.258544921875 validation acc:0.12999999523162842\n",
      "step:100 train loss:35.18122100830078 validation acc:0.8999999761581421\n",
      "step:200 train loss:41.98802185058594 validation acc:0.9200000166893005\n",
      "step:300 train loss:38.32001876831055 validation acc:0.9300000071525574\n",
      "step:400 train loss:21.2425537109375 validation acc:0.949999988079071\n",
      "step:500 train loss:28.345348358154297 validation acc:0.9300000071525574\n",
      "step:600 train loss:42.71527099609375 validation acc:0.8999999761581421\n",
      "step:700 train loss:32.13701248168945 validation acc:0.8899999856948853\n",
      "step:800 train loss:43.98413848876953 validation acc:0.8999999761581421\n",
      "step:900 train loss:23.53229522705078 validation acc:0.9300000071525574\n",
      "step:1000 train loss:56.62244415283203 validation acc:0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_input_data=tf.placeholder(tf.float32,[None,784],name='x_input_data')\n",
    "x_input_label=tf.placeholder(tf.float32,[None,10],name='x_input_label')\n",
    "w=tf.Variable(tf.zeros([784,10]))\n",
    "b=tf.Variable(tf.zeros([10]))\n",
    "y_out=tf.nn.softmax(tf.matmul(x_input_data,w)+b)\n",
    "cross_entropy=-tf.reduce_sum(x_input_label*tf.log(y_out))\n",
    "train_step=tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "init_op=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(1001):\n",
    "        batch_x_data,batch_x_label=mnist.train.next_batch(100)\n",
    "        correct_prediction=tf.equal(tf.argmax(y_out,1),tf.argmax(batch_x_label,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_prediction,'float'))\n",
    "        [acc,y_out_train,train_loss,_]=sess.run([accuracy,y_out,cross_entropy,train_step,],\n",
    "                                            feed_dict={x_input_data:batch_x_data,x_input_label:batch_x_label})\n",
    "        if step%100==0:\n",
    "            print(\"step:{} train loss:{} validation acc:{}\".format(step,train_loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST进阶-构建一个多层卷积网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重初始化\n",
    "\n",
    "为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积和池化\n",
    "\n",
    "TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.78\n",
      "step 200, training accuracy 0.92\n",
      "step 300, training accuracy 0.9\n",
      "step 400, training accuracy 0.94\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.92\n",
      "step 1100, training accuracy 0.94\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 1\n",
      "step 1600, training accuracy 1\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 0.92\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "sess = tf.InteractiveSession()\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "          x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用自己的理解重写一遍\n",
    "将网络写成类，将训练写成函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-cb97af51b565>, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-cb97af51b565>\"\u001b[0;36m, line \u001b[0;32m81\u001b[0m\n\u001b[0;31m    inputData=batch[0],\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def MNIST_NET(object):\n",
    "    def __init__(self,labelNum,keep_prob):\n",
    "        self.labelNum=lebelNum\n",
    "        self.learningRate=learningRate\n",
    "        self.keep_prob=keep_prob\n",
    "    def weight_variable(self,shape,name):\n",
    "        return tf.Variable(tf.truncated_normal(shape=shape,stddev=0.1),name=name)\n",
    "    def bias_variable(self,shape,anme):\n",
    "        return tf.constant(0.1,shape=shape,name=name)\n",
    "        \n",
    "    def interface(self,inputData):\n",
    "        #将输入reshape成合适的shape\n",
    "        x_image=tf.reshape(inputData,shape=[-1,28,28,1])\n",
    "        \n",
    "        #第一层卷积层\n",
    "        W_conv1=self.weight_variable([5,5,1,32],'W_conv1')\n",
    "        b_conv1=self.bias_variable([32],'b_conv1')\n",
    "        conv1=tf.nn.bias_add(tf.nn.conv2d(\n",
    "            x_image,W_conv1,strides=[1,1,1,1],padding='SAME'),b_conv1)\n",
    "        conv1_relu=tf.nn.relu(conv1,name='conv1_relu')\n",
    "        conv1_maxpool=tf.nn.max_pool(\n",
    "            conv1_relu,ksize=[1,2,2,1],strides=[1,2,2,1],\n",
    "            padding='SAME',name='conv1_maxpool')\n",
    "        \n",
    "        #第二层卷积层\n",
    "        W_conv2=self.weight_variable([5,5,32,64],'W_conv2')\n",
    "        b_conv2=self.bias_variable([64],'b_conv2')\n",
    "        conv2=tf.nn.bias_add(tf.nn.conv2d(\n",
    "            conv1_maxpool,W_conv2,strides=[1,1,1,1],padding='SAME'),b_conv2)\n",
    "        conv2_relu=tf.nn.relu(conv2,name='conv2_relu')\n",
    "        conv2_maxpool=tf.nn.max_pool(\n",
    "            conv2_relu,ksize=[1,2,2,1],strides=[1,2,2,1],\n",
    "            padding='SAME',name='conv2_maxpool')\n",
    "        \n",
    "        #密集连接层(全连接层)\n",
    "        conv2_maxpool_shape=conv2_maxpool.get_shape().as_list()\n",
    "        nodesNum=conv2_maxpool_shape[1]*conv2_maxpool_shape[2]*conv2_maxpool_shape[3]\n",
    "        conv2_maxpool_reshape=tf.reshape(conv2_maxpool,shape=[-1,nodesNum])\n",
    "        W_fc1=self.weight_variable([nodesNum,1024],'W_fc1')\n",
    "        b_fc1=self.bias_variable([1024],'b_fc1')\n",
    "        fc1=tf.nn.relu(tf.matmul(conv2_maxpool_reshape,W_fc1)+b_fc1,name='fc1')\n",
    "        #防止过拟合，使用dropout\n",
    "        fc1_drop=tf.nn.dropout(fc1,self.keep_prob,name='fc1_drop')\n",
    "        \n",
    "        #输出层(全连接层)\n",
    "        W_fc2=self.weight_variable([1024,self.labelNum],'W_fc2')\n",
    "        b_fc2=self.bias_variable([self.labelNum],'b_fc2')\n",
    "        logits=tf.add(tf.matmul(fc1_drop,W_fc2),b_fc2,name='logits')\n",
    "        return logits\n",
    "    \n",
    "    def getAccuracy(self,logits,inputlabel):\n",
    "        logits_softmax=tf.nn.softmax(logits)\n",
    "        logitsEqual=tf.equal(tf.argmax(logits_softmax,1),tf.argmax(inputlabel,1))\n",
    "        acc=tf.reduce_mean(tf.cast(logitsEqual,'float'),name='accuracy')\n",
    "        return acc\n",
    "    \n",
    "def train():\n",
    "    labelNum=10\n",
    "    learningRate=0.01\n",
    "    keep_prob=1\n",
    "    mynet=MNIST_NET(labelNum,keep_prob)\n",
    "    inputData=tf.placeholder('float',shape=[None,28,28,1])\n",
    "    inputlabel=tf.placeholder('float',shape=[None,labelNum])\n",
    "    logits=mynet.interface(inputData)\n",
    "    acc=mynet.getAccuracy(logits,inputlabel)\n",
    "    cross_entropy=tf.nn.softmax_cross_entropy_with_logits(labels=inputlabel,\n",
    "                                                         logits=logits,\n",
    "                                                         name='cross_entropy')\n",
    "    cross_entropy_mean=tf.reduce_mean(cross_entropy,name='cross_entropy_mean')\n",
    "    train_optimizer=tf.train.GradientDescentOptimizer(learningRate).minimize(cross_entropy_mean,\n",
    "                                                                            name='train_optimizer')\n",
    "    with tf.Session() as sess:\n",
    "        print('-------start training------')\n",
    "        tf.run(tf.global_variables_initializer())\n",
    "        for i in range(2001):\n",
    "            batch = mnist.train.next_batch(50)\n",
    "            [logits_val,acc_val,loss_val,_]=sess.run([logits,acc,cross_entropy_mean,train_optimizer],\n",
    "                                           feed_dict={\n",
    "                                               inputData=batch[0],\n",
    "                                               inputlabel=batch[1]\n",
    "                                           })\n",
    "            if i%100 == 0:\n",
    "                print('step:%d  loss:%f  acc:%f'%(step,loss_val,acc_val))\n",
    "        \n",
    "        print('-------start validating------')\n",
    "        acc_val=sess.run([acc],\n",
    "                         feed_dict={\n",
    "                             inputData=mnist.test.images,\n",
    "                             inputlabel=mnist.test.labels\n",
    "                         })\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
