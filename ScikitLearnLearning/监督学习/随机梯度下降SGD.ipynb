{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent ( 随机梯度下降 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) ( 随机梯度下降（ SGD ） ) 是一种简单但非常有效的方法，用于在诸如（线性）支持向量机和 逻辑回归 之类的凸损失函数下的线性分类器的辨别学习。即使 SGD 已经在机器学习社区中长期存在，但最近在大规模学习的背景下已经受到了相当多的关注。\n",
    "SGD 已成功应用于文本分类和自然语言处理中经常遇到的大规模和稀疏机器学习问题。\n",
    "SGD 已成功应用于文本分类和自然语言处理中经常遇到的 large-scale and sparse ( 大规模和稀疏 ) 机器学习问题。鉴于数据稀疏，本模块中的分类器容易扩展到具有 10^5 个以上训练样本和超过10^5个特征的问题。<br>\n",
    "随机梯度下降的优点是：\n",
    "* 效率。\n",
    "* 易于实施（很多机会进行代码调优）。<br>\n",
    "随机梯度下降的缺点包括：\n",
    "* SGD 需要一些 hyperparameters  ( 超参数 ) ，如 regularization parameter ( 正则化参数 ) 和迭代次数。\n",
    "* SGD 对特征缩放敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenshao/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的 loss function ( 损失函数 ) 可以通过  loss parameter ( 损失参数 ) 设置。 SGDClassifier 支持以下损失函数：\n",
    "* loss=\"hinge\": (soft-margin) linear Support Vector Machine, ( 线性支持向量机 )\n",
    "* loss=\"modified_huber\": smoothed hinge loss, ( 平滑的 hinge 损失 )\n",
    "* loss=\"log\": logistic regression, ( 逻辑回归 )\n",
    "* and all regression losses below. ( 所有回归损失 )\n",
    "前两个 loss functions ( 损失函数 ) 是 lazy (  懒惰 )的，如果一个例子违反了 margin constraint ( 边界约束 ) ，则它们仅更新模型参数，这使得训练非常有效，并且即使使用了 L2 penalty ( L2 惩罚 ) ，也可能导致较差的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenshao/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00459185,  0.99540815]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\").fit(X, y)\n",
    "clf.predict_proba([[1., 1.]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** concrete penalty ( 具体惩罚 )** 可以通过 penalty parameter ( 惩罚参数 ) 来设定。 SGD 支持以下处罚：\n",
    "* penalty=\"l2\": L2 norm penalty on coef_.\n",
    "* penalty=\"l1\": L1 norm penalty on coef_.\n",
    "* penalty=\"elasticnet\": Convex combination of L2 and L1; (1 - l1_ratio) * L2 + l1_ratio * L1.\n",
    "<br>默认设置为 penalty =“l2” 。 L1 惩罚获得 sparse solutions ( 稀疏解 ) ，将大多数系数推到零。在存在高度相关的属性的情况下，Elastic Net ( 弹性网 ) 解决了 L1 惩罚的一些缺陷。参数 l1_ratio 控制 L1 和 L2 惩罚的凸组合。\n",
    "SGDClassifier 通过在 “one versus all”（OVA） 方案中组合多个二进制分类器来支持多类分类。对于 K 类中的每一个，学习了二进制分类器，以区别于所有其他 K-1 类。在测试时，我们计算每个分类器的置信度分数（即与超平面的有符号距离），并选择具有最高置信度的类。下图显示了虹膜数据集的 OVA 方法。虚线代表三个 OVA 分类器;背景颜色显示由三个分类器引起的决策表面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression ( 回归 )\n",
    "SGDRegressor 类实现了一个简单的随机梯度下降学习程序，它支持不同的损失函数和惩罚，以适应线性回归模型。 SGDRegressor 非常适合大量培训样本（> 10.000）的回归问题，对于其他问题，我们建议使用 Ridge ， Lasso 或 ElasticNet 。<br>\n",
    "具体的损失函数可以通过损失参数设置。 SGDRegressor 支持以下损失函数：<br>\n",
    "* loss=\"squared_loss\": Ordinary least squares,\n",
    "* loss=\"huber\": Huber loss for robust regression,\n",
    "* loss=\"epsilon_insensitive\": linear Support Vector Regression.\n",
    "* Huber 和 epsilon-insensitive 不敏感损失函数可用于 robust regression (  鲁棒回归 )。不敏感区域的宽度必须通过参数 epsilon 来指定。该参数取决于目标变量的比例。\n",
    "SGDRegressor 支持平均 SGD 作为 SGDClassifier 。可以通过设置 “average = True” 来启用平均。\n",
    "对于具有 squared loss ( 平方损失 ) 和 l2 penalty (  l2 惩罚 )的回归，具有平均策略的 SGD 的另一变体可用于随机平均梯度（ SAG ）算法，作为 Ridge 中的求解器。<br>\n",
    "Stochastic Gradient Descent for sparse data ( 稀疏数据随机梯度下降 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips on Practical Use ( 实用技巧 )\n",
    "随机梯度下降对特征缩放很敏感，因此强烈建议您扩展数据。例如，将输入向量 X 上的每个属性缩放到 [0,1] 或 [-1，+ 1] ，或将其标准化为平均值 0 和方差 1 . \n",
    "### 注意，必须将相同的缩放应用于测试向量获得有意义的结果。这可以使用 StandardScaler 轻松完成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=100*np.random.rand(100,2)+30\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)  # apply same transformation to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00804135, -0.14177623],\n",
       "       [ 0.50850806, -0.11211407],\n",
       "       [-0.20080773, -0.88702953],\n",
       "       [ 0.46508921,  1.16636257],\n",
       "       [ 1.19985646,  0.51674059],\n",
       "       [-0.79271408, -1.21601804],\n",
       "       [-0.32932005, -0.95095443],\n",
       "       [-0.12723639,  0.84228326],\n",
       "       [ 1.39927398,  1.11073041],\n",
       "       [ 0.75213075, -1.10910095],\n",
       "       [ 1.24036887,  1.45802774],\n",
       "       [ 1.13353818,  1.46924215],\n",
       "       [ 1.01408706,  0.98623497],\n",
       "       [-0.57510198,  1.01810899],\n",
       "       [-1.10572582,  1.03230095],\n",
       "       [ 1.19495996, -0.50921504],\n",
       "       [-0.85023192, -0.20276057],\n",
       "       [ 0.75998136, -1.49276673],\n",
       "       [ 0.87620844,  0.15336095],\n",
       "       [-1.65526066,  0.1364395 ],\n",
       "       [ 0.61123055,  0.68249384],\n",
       "       [ 0.76820165,  1.51895256],\n",
       "       [-0.07324067, -1.34902349],\n",
       "       [ 0.22170115,  0.45317919],\n",
       "       [ 0.45023639,  1.6022048 ],\n",
       "       [-1.15975562,  1.43419277],\n",
       "       [ 0.37408172, -0.97391246],\n",
       "       [-1.51179764, -0.82673572],\n",
       "       [ 1.31815716,  0.65078189],\n",
       "       [ 1.36073557,  1.38619444],\n",
       "       [-0.61714314, -0.81359038],\n",
       "       [-0.27469821, -0.03108349],\n",
       "       [ 0.97190516, -1.33955986],\n",
       "       [-0.61474122, -1.19401314],\n",
       "       [-1.24106806,  1.17644196],\n",
       "       [ 0.43393866, -0.7767871 ],\n",
       "       [-1.47578751,  1.62950247],\n",
       "       [ 1.07054787,  1.5953262 ],\n",
       "       [-0.4335579 ,  1.55349124],\n",
       "       [-0.41064971, -1.14261606],\n",
       "       [ 0.22712345,  0.92927859],\n",
       "       [-1.62627154, -1.1612012 ],\n",
       "       [-1.1495512 ,  1.33811972],\n",
       "       [ 1.47161625, -0.92559481],\n",
       "       [-0.75397603, -0.17085911],\n",
       "       [ 0.11454646, -0.69836317],\n",
       "       [ 0.97538261,  0.59802714],\n",
       "       [ 0.31294789,  0.17611764],\n",
       "       [ 1.22246069, -0.15631543],\n",
       "       [ 1.39126422, -1.51197362],\n",
       "       [ 0.81156059, -0.59377243],\n",
       "       [ 1.33091323,  0.53152734],\n",
       "       [-1.61628249, -0.64695127],\n",
       "       [ 0.70898933, -1.1595083 ],\n",
       "       [-1.18721229, -1.45797239],\n",
       "       [-0.28393606,  1.26015811],\n",
       "       [ 0.84829535,  0.45247042],\n",
       "       [ 0.63076591,  1.46132046],\n",
       "       [ 1.54827006, -1.45279512],\n",
       "       [ 0.82293754, -1.21080699],\n",
       "       [ 1.11124043, -0.54745904],\n",
       "       [-0.63064418, -0.97220724],\n",
       "       [-1.1117525 , -0.37715039],\n",
       "       [ 0.47452736,  0.19689317],\n",
       "       [ 1.14840394,  0.43357156],\n",
       "       [-0.33807881,  1.25356717],\n",
       "       [-1.51240054, -0.35713042],\n",
       "       [ 0.74651745, -1.80288469],\n",
       "       [ 0.97201731, -1.53883555],\n",
       "       [ 0.59909003,  1.04994526],\n",
       "       [-0.8497765 , -0.27966932],\n",
       "       [ 0.05415646,  0.87859846],\n",
       "       [-0.98742868, -0.38147705],\n",
       "       [-1.18071581, -1.41877488],\n",
       "       [-0.60684367,  0.56759598],\n",
       "       [ 1.33285231, -0.01017138],\n",
       "       [-1.35145672, -0.62349397],\n",
       "       [ 1.26538321,  0.5386529 ],\n",
       "       [ 1.32691659,  0.84181683],\n",
       "       [ 1.41777217, -0.23057165],\n",
       "       [ 1.51221581, -1.45455062],\n",
       "       [ 0.56213281,  0.73920292],\n",
       "       [-1.63412992,  1.65898922],\n",
       "       [-1.66857472, -1.28563104],\n",
       "       [ 0.03136022,  1.29757025],\n",
       "       [-0.38972515,  0.40118463],\n",
       "       [-0.53985381,  0.36086592],\n",
       "       [ 0.75510286, -0.59609222],\n",
       "       [-0.5770054 ,  0.83792929],\n",
       "       [-1.17637869,  0.31827595],\n",
       "       [-1.34739172, -0.75710468],\n",
       "       [-1.29738919, -0.54767701],\n",
       "       [-1.43980019,  0.9740355 ],\n",
       "       [-0.8621573 , -1.43528105],\n",
       "       [ 0.34791801, -0.88481541],\n",
       "       [-0.80547425, -1.24574713],\n",
       "       [ 0.66142004, -0.69635244],\n",
       "       [-0.67717902,  0.94045509],\n",
       "       [-1.22541835,  0.79295933],\n",
       "       [-1.57715445, -0.74347395]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您的属性具有内在尺度（例如字频或指标特征），则不需要缩放。\n",
    " \n",
    "* 找到合理的正则化项 最好使用 GridSearchCV ，通常在 10.0 ** - np.arange(1,7) 范围内。\n",
    "经验性地，我们发现 SGD 在观察大约后收敛。  10^6 训练样本。因此，对于迭代次数的合理的第一猜测是 n_iter = np.ceil(10 ** 6/n) ，其中 n * 是训练集的大小。\n",
    "* 如果将 SGD 应用于使用 PCA 提取的特征，我们发现将特征值扩展为常数 c 通常是明智的，以使训练数据的平均 L2 范数等于 1 。\n",
    "* 我们发现平均 SGD 最适合于更多的功能和更高的 eta0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
