{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel ridge regression ( 内核岭回归 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel ridge regression (KRR)  ( 内核岭回归（ KRR ） )  [M2012] 将  Ridge Regression ( 岭回归 ) （线性最小二乘法与 l2 规范正则化）与内核技巧相结合。因此，它将学习由各个内核和数据引起的空间中的线性函数。对于非线性内核，这对应于原始空间中的非线性函数。\n",
    "由 KernelRidge 学习的模型的形式与支持向量回归  ( SVR )是一样的。然而，使用不同的损耗函数：KRR 使用平方误差损失，而支持向量回归使用  -insensitive loss ( ε-不敏感损失 )，两者都结合 l2 正则化。与 SVR 相反，拟合 KernelRidge 可以以封闭形式完成，对于中型数据集通常更快。另一方面，学习的模型是非稀疏的，因此比 SVR 慢，在 SVR 中，在预测时间中学习了 的稀疏模型。\n",
    "下图比较了人造数据集上的 KernelRidge 和 SVR ，它由一个正弦目标函数和加到每个第五个数据点的强噪声组成。绘制了 KernelRidge 和 SVR 的学习模型，其中使用网格搜索优化了 RBF 内核的复杂性/正则化和带宽。学习功能非常相似;但是，配合 KernelRidge 约为。比拟合 SVR 快两倍（都是 grid-search  ( 网格搜索 ) ）。然而，由于 SVR 只学习了一个稀疏模型，所以 SVR 的预测值超过了 10 万个目标值的三倍以上。 100 个训练数据点中的 1/3 作为支持向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图显示不同大小训练集的 KernelRidge 和 SVR 的拟合和预测时间。 对于中型训练集（小于 1000 个样本），拟合 KernelRidge 比 SVR 快; 然而，对于较大的训练集 SVR 比例更好。 关于预测时间，由于学习的稀疏解， SVR 对于训练集的所有大小都比 KernelRidge 快。 注意，稀疏度和预测时间取决于 SVR 的参数  和 C ;  将对应于密集模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
